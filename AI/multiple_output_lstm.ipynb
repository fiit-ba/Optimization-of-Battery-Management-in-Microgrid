{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8abaaf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from keras.models import Input, Model, Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Concatenate, SimpleRNN, Masking, Flatten\n",
    "from keras import losses\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71292a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n",
    "    n_features = ts.shape[1]\n",
    "    X, Y = [], []\n",
    "    if len(ts) - lag <= 0:\n",
    "        X.append(ts)\n",
    "    else:\n",
    "        for i in range(len(ts) - lag - n_ahead):\n",
    "            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n",
    "            X.append(ts[i:(i + lag)])\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    X = np.reshape(X, (X.shape[0], lag, n_features))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d2cad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNMultistepModel():\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        X, \n",
    "        Y, \n",
    "        n_outputs,\n",
    "        n_lag,\n",
    "        n_ft,\n",
    "        n_layer,\n",
    "        batch,\n",
    "        epochs, \n",
    "        lr,\n",
    "        Xval=None,\n",
    "        Yval=None,\n",
    "        mask_value=-999.0,\n",
    "        min_delta=0.001,\n",
    "        patience=5\n",
    "    ):\n",
    "        lstm_input = Input(shape=(n_lag, n_ft))\n",
    "\n",
    "        # Series signal \n",
    "        lstm_layer = LSTM(n_layer, activation='relu')(lstm_input)\n",
    "\n",
    "        x = Dense(n_outputs)(lstm_layer)\n",
    "        \n",
    "        self.model = Model(inputs=lstm_input, outputs=x)\n",
    "        self.batch = batch \n",
    "        self.epochs = epochs\n",
    "        self.n_layer=n_layer\n",
    "        self.lr = lr \n",
    "        self.Xval = Xval\n",
    "        self.Yval = Yval\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.mask_value = mask_value\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "\n",
    "    def trainCallback(self):\n",
    "        return EarlyStopping(monitor='loss', patience=self.patience, min_delta=self.min_delta)\n",
    "\n",
    "    def train(self):\n",
    "        # Getting the untrained model \n",
    "        empty_model = self.model\n",
    "        \n",
    "        # Initiating the optimizer\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "        # Compiling the model\n",
    "        empty_model.compile(loss=losses.MeanAbsoluteError(), optimizer=optimizer)\n",
    "\n",
    "        if (self.Xval is not None) & (self.Yval is not None):\n",
    "            history = empty_model.fit(\n",
    "                self.X, \n",
    "                self.Y, \n",
    "                epochs=self.epochs, \n",
    "                batch_size=self.batch, \n",
    "                validation_data=(self.Xval, self.Yval), \n",
    "                shuffle=False,\n",
    "                callbacks=[self.trainCallback()]\n",
    "            )\n",
    "        else:\n",
    "            history = empty_model.fit(\n",
    "                self.X, \n",
    "                self.Y, \n",
    "                epochs=self.epochs, \n",
    "                batch_size=self.batch,\n",
    "                shuffle=False,\n",
    "                callbacks=[self.trainCallback()]\n",
    "            )\n",
    "        \n",
    "        # Saving to original model attribute in the class\n",
    "        self.model = empty_model\n",
    "        \n",
    "        # Returning the training history\n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35bba096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/data_processed.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y/%m/%d %H:%M')\n",
    "df['production_usage'] = df['production_usage'].fillna(df['production_usage'].median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1efdf885",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 24\n",
    "n_ahead = 1\n",
    "test_share = 0.1\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "n_layer = 10\n",
    "features_final = [ 'consumption_usage','price', 'isWeekend', 'isHoliday',\n",
    "                  'temp', 'wind','day_cos', 'day_sin',\n",
    "                  'month_cos', 'month_sin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f4261d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df[features_final]\n",
    "nrows = ts.shape[0]\n",
    "train = ts[0:int(nrows * (1 - test_share))]\n",
    "test = ts[int(nrows * (1 - test_share)):]\n",
    "train_mean = train.mean()\n",
    "train_std = train.std()\n",
    "train = (train - train_mean) / train_std\n",
    "test = (test - train_mean) / train_std\n",
    "ts_s = pd.concat([train, test])\n",
    "X, Y = create_X_Y(ts_s.values, lag=lag, n_ahead=n_ahead)\n",
    "n_ft = X.shape[2]\n",
    "Xtrain, Ytrain = X[0:int(X.shape[0] * (1 - test_share))], Y[0:int(X.shape[0] * (1 - test_share))]\n",
    "Xval, Yval = X[int(X.shape[0] * (1 - test_share)):], Y[int(X.shape[0] * (1 - test_share)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b917340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "493/493 [==============================] - 8s 13ms/step - loss: 0.9076 - val_loss: 1.4170\n",
      "Epoch 2/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.6872 - val_loss: 1.0726\n",
      "Epoch 3/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.5790 - val_loss: 0.9315\n",
      "Epoch 4/50\n",
      "493/493 [==============================] - 6s 11ms/step - loss: 0.5084 - val_loss: 0.8226\n",
      "Epoch 5/50\n",
      "493/493 [==============================] - 6s 13ms/step - loss: 0.4568 - val_loss: 0.7263\n",
      "Epoch 6/50\n",
      "493/493 [==============================] - 7s 14ms/step - loss: 0.4158 - val_loss: 0.6446\n",
      "Epoch 7/50\n",
      "493/493 [==============================] - 7s 14ms/step - loss: 0.3794 - val_loss: 0.5752\n",
      "Epoch 8/50\n",
      "493/493 [==============================] - 8s 16ms/step - loss: 0.3452 - val_loss: 0.5180\n",
      "Epoch 9/50\n",
      "493/493 [==============================] - 8s 16ms/step - loss: 0.3143 - val_loss: 0.4703\n",
      "Epoch 10/50\n",
      "493/493 [==============================] - 8s 16ms/step - loss: 0.2881 - val_loss: 0.4318\n",
      "Epoch 11/50\n",
      "493/493 [==============================] - 8s 16ms/step - loss: 0.2657 - val_loss: 0.3987\n",
      "Epoch 12/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.2463 - val_loss: 0.3676\n",
      "Epoch 13/50\n",
      "493/493 [==============================] - 6s 11ms/step - loss: 0.2290 - val_loss: 0.3374\n",
      "Epoch 14/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.2126 - val_loss: 0.3083\n",
      "Epoch 15/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.1969 - val_loss: 0.2812\n",
      "Epoch 16/50\n",
      "493/493 [==============================] - 7s 14ms/step - loss: 0.1815 - val_loss: 0.2575\n",
      "Epoch 17/50\n",
      "493/493 [==============================] - 6s 13ms/step - loss: 0.1674 - val_loss: 0.2378\n",
      "Epoch 18/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.1552 - val_loss: 0.2208\n",
      "Epoch 19/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.1448 - val_loss: 0.2031\n",
      "Epoch 20/50\n",
      "493/493 [==============================] - 6s 11ms/step - loss: 0.1359 - val_loss: 0.1886\n",
      "Epoch 21/50\n",
      "493/493 [==============================] - 7s 14ms/step - loss: 0.1282 - val_loss: 0.1763\n",
      "Epoch 22/50\n",
      "493/493 [==============================] - 6s 13ms/step - loss: 0.1221 - val_loss: 0.1655\n",
      "Epoch 23/50\n",
      "493/493 [==============================] - 6s 13ms/step - loss: 0.1175 - val_loss: 0.1536\n",
      "Epoch 24/50\n",
      "493/493 [==============================] - 7s 13ms/step - loss: 0.1138 - val_loss: 0.1421\n",
      "Epoch 25/50\n",
      "493/493 [==============================] - 6s 13ms/step - loss: 0.1106 - val_loss: 0.1299\n",
      "Epoch 26/50\n",
      "493/493 [==============================] - 6s 13ms/step - loss: 0.1078 - val_loss: 0.1195\n",
      "Epoch 27/50\n",
      "493/493 [==============================] - 6s 13ms/step - loss: 0.1051 - val_loss: 0.1090\n",
      "Epoch 28/50\n",
      "493/493 [==============================] - 6s 13ms/step - loss: 0.1028 - val_loss: 0.0988\n",
      "Epoch 29/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.1006 - val_loss: 0.0895\n",
      "Epoch 30/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.0987 - val_loss: 0.0814\n",
      "Epoch 31/50\n",
      "493/493 [==============================] - 6s 13ms/step - loss: 0.0970 - val_loss: 0.0740\n",
      "Epoch 32/50\n",
      "493/493 [==============================] - 6s 13ms/step - loss: 0.0954 - val_loss: 0.0684\n",
      "Epoch 33/50\n",
      "493/493 [==============================] - 6s 11ms/step - loss: 0.0940 - val_loss: 0.0635\n",
      "Epoch 34/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.0927 - val_loss: 0.0591\n",
      "Epoch 35/50\n",
      "493/493 [==============================] - 6s 13ms/step - loss: 0.0916 - val_loss: 0.0564\n",
      "Epoch 36/50\n",
      "493/493 [==============================] - 6s 13ms/step - loss: 0.0906 - val_loss: 0.0544\n",
      "Epoch 37/50\n",
      "493/493 [==============================] - 6s 11ms/step - loss: 0.0897 - val_loss: 0.0510\n",
      "Epoch 38/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.0890 - val_loss: 0.0487\n",
      "Epoch 39/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.0883 - val_loss: 0.0468\n",
      "Epoch 40/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.0877 - val_loss: 0.0454\n",
      "Epoch 41/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.0871 - val_loss: 0.0446\n",
      "Epoch 42/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.0867 - val_loss: 0.0437\n",
      "Epoch 43/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.0862 - val_loss: 0.0422\n",
      "Epoch 44/50\n",
      "493/493 [==============================] - 6s 13ms/step - loss: 0.0859 - val_loss: 0.0415\n",
      "Epoch 45/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.0855 - val_loss: 0.0408\n",
      "Epoch 46/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.0852 - val_loss: 0.0405\n",
      "Epoch 47/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.0849 - val_loss: 0.0401\n",
      "Epoch 48/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.0846 - val_loss: 0.0398\n",
      "Epoch 49/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.0844 - val_loss: 0.0391\n",
      "Epoch 50/50\n",
      "493/493 [==============================] - 6s 12ms/step - loss: 0.0841 - val_loss: 0.0394\n"
     ]
    }
   ],
   "source": [
    "model = NNMultistepModel(\n",
    "    X=Xtrain,\n",
    "    Y=Ytrain,\n",
    "    n_outputs=n_ahead,\n",
    "    n_lag=lag,\n",
    "    n_ft=n_ft,\n",
    "    n_layer=n_layer,\n",
    "    batch=batch_size,\n",
    "    epochs=epochs, \n",
    "    lr=lr,\n",
    "    Xval=Xval,\n",
    "    Yval=Yval,\n",
    ")\n",
    "history = model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
